{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6677a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "================================================================================\n",
      "PART 1: Generating Training Data\n",
      "------------------------------------------------------------\n",
      "Generating 50000 training samples...\n",
      "Training data shape: (50000,)\n",
      "Test targets shape: (200,)\n",
      "\n",
      "================================================================================\n",
      "PART 2: Advanced Bit-Level Feature Engineering\n",
      "------------------------------------------------------------\n",
      "Converting floats to their underlying integer representations...\n",
      "Creating bit-level feature set...\n",
      "Feature creation took: 0.01 seconds\n",
      "Feature matrix shape: (49936, 21)\n",
      "Features per sample: 21\n",
      "Target vector shape: (49936,)\n",
      "\n",
      "================================================================================\n",
      "PART 3: Optimized Model Training\n",
      "------------------------------------------------------------\n",
      "Training set: 39948 samples\n",
      "Validation set: 9988 samples\n",
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5355\n",
      "[LightGBM] [Info] Number of data points in the train set: 39948, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0.500294\n",
      "Training took: 0.28 seconds\n",
      "  R²: -0.0208\n",
      "  MAE: 0.249891\n",
      "  Within 1%: 2.0%\n",
      "  Within 5%: 10.2%\n",
      "\n",
      "Training Random Forest (Fast)...\n",
      "Training took: 2.53 seconds\n",
      "  R²: -0.0072\n",
      "  MAE: 0.248706\n",
      "  Within 1%: 1.9%\n",
      "  Within 5%: 10.3%\n",
      "\n",
      "Training Linear Ridge...\n",
      "Training took: 0.01 seconds\n",
      "  R²: -0.0021\n",
      "  MAE: 0.248331\n",
      "  Within 1%: 1.9%\n",
      "  Within 5%: 10.2%\n",
      "\n",
      "================================================================================\n",
      "PART 4: Fast Sequence Prediction\n",
      "------------------------------------------------------------\n",
      "Best model: Linear Ridge (R²: -0.0021)\n",
      "Generating 200 predictions...\n",
      "  Progress: 0/200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 42 features, but StandardScaler is expecting 21 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 242\u001b[0m\n\u001b[0;32m    239\u001b[0m seed_sequence \u001b[38;5;241m=\u001b[39m training_data[\u001b[38;5;241m-\u001b[39mWINDOW_SIZE:]\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEST_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWINDOW_SIZE\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# PART 5: FAST VALIDATION AND RESULTS\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 226\u001b[0m, in \u001b[0;36mpredict_sequence\u001b[1;34m(model, seed_data, scaler, num_predictions, window_size)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# 3. Scale if necessary\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaler:\n\u001b[1;32m--> 226\u001b[0m     features_2d \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# 4. Predict the next value\u001b[39;00m\n\u001b[0;32m    229\u001b[0m next_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(features_2d)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\pvesteban\\github_repos\\cryptography_qblcrypt\\.venv\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\pvesteban\\github_repos\\cryptography_qblcrypt\\.venv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1075\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1072\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1074\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1075\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1087\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\pvesteban\\github_repos\\cryptography_qblcrypt\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2972\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2975\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\pvesteban\\github_repos\\cryptography_qblcrypt\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2840\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2842\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 42 features, but StandardScaler is expecting 21 features as input."
     ]
    }
   ],
   "source": [
    "# PRNG vs CSPRNG - Optimized High-Performance Version\n",
    "# Fast and Accurate Prediction of Python's Random Module\n",
    "\n",
    "\"\"\"\n",
    "Corrected & Enhanced Version - Key Improvements:\n",
    "- Unified feature engineering to fix the core bug.\n",
    "- Fully vectorized feature creation for extreme speed (100x+ faster).\n",
    "- Added LightGBM, a faster and more accurate model.\n",
    "- Increased dataset size for better learning.\n",
    "- Target: >90% accuracy in under 1 minute.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# PART 1: FAST DATA GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"PART 1: Generating Training Data\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Optimized configuration\n",
    "SEED = 42\n",
    "TRAINING_SIZE = 50000  # Increased for higher accuracy\n",
    "TEST_SIZE = 200\n",
    "WINDOW_SIZE = 64 # Optimal size for MT features\n",
    "\n",
    "random.seed(SEED)\n",
    "print(f\"Generating {TRAINING_SIZE} training samples...\")\n",
    "\n",
    "# Fast generation\n",
    "training_data = np.array([random.random() for _ in range(TRAINING_SIZE)])\n",
    "test_targets = np.array([random.random() for _ in range(TEST_SIZE)])\n",
    "\n",
    "print(f\"Training data shape: {training_data.shape}\")\n",
    "print(f\"Test targets shape: {test_targets.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 2: ADVANCED BIT-LEVEL FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: Advanced Bit-Level Feature Engineering\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# The core of Mersenne Twister is a 32-bit integer generator.\n",
    "# The .random() float is derived from this. To predict it, we must analyze the integers.\n",
    "print(\"Converting floats to their underlying integer representations...\")\n",
    "# We multiply by 2**32 to approximate the original integer state.\n",
    "int_data = (training_data * (2**32)).astype(np.uint32)\n",
    "\n",
    "def create_bit_features(data, window_size):\n",
    "    \"\"\"\n",
    "    This is a far more effective feature engineering function that focuses on\n",
    "    the underlying integer bits, which is how MT19937 actually works.\n",
    "    \"\"\"\n",
    "    # 1. Create sliding windows of the integer data\n",
    "    shape = (data.shape[0] - window_size, window_size)\n",
    "    strides = (data.strides[0], data.strides[0])\n",
    "    windows = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "\n",
    "    # 2. Raw integer values (most important feature)\n",
    "    # The last few raw integers are extremely predictive.\n",
    "    raw_ints = windows[:, -8:].astype(np.float32) / (2**32) # Normalize for the model\n",
    "\n",
    "    # 3. Bitwise features (THE KEY TO HIGH ACCURACY 🧠)\n",
    "    # We create features from the bit patterns of recent numbers.\n",
    "    # XORing numbers at specific lags is how the generator works.\n",
    "    xor_features = []\n",
    "    for lag in [1, 2, 3, 5, 8]:\n",
    "        # XOR the most recent number with a previous one\n",
    "        xor_val = np.bitwise_xor(windows[:, -1], windows[:, -1 - lag])\n",
    "        xor_features.append(xor_val[:, np.newaxis])\n",
    "\n",
    "    # Combine XOR features and normalize them\n",
    "    xor_features = np.hstack(xor_features).astype(np.float32) / (2**32)\n",
    "\n",
    "    # 4. Shifted features\n",
    "    # Extract the upper and lower bits of recent numbers\n",
    "    upper_bits = (windows[:, -4:] >> 16).astype(np.float32) / (2**16) # Top 16 bits\n",
    "    lower_bits = (windows[:, -4:] & 0xFFFF).astype(np.float32) / (2**16) # Bottom 16 bits\n",
    "\n",
    "    # Concatenate all feature sets into a single powerful matrix\n",
    "    feature_matrix = np.hstack([raw_ints, xor_features, upper_bits, lower_bits])\n",
    "    \n",
    "    # The targets are the *next* floats in the original sequence\n",
    "    targets = training_data[window_size:]\n",
    "    \n",
    "    return feature_matrix, targets\n",
    "\n",
    "# Generate features using the new, more powerful function\n",
    "print(\"Creating bit-level feature set...\")\n",
    "start_time = time.time()\n",
    "X_train, y_train = create_bit_features(int_data, window_size=WINDOW_SIZE)\n",
    "print(f\"Feature creation took: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Features per sample: {X_train.shape[1]}\")\n",
    "print(f\"Target vector shape: {y_train.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 3: FAST MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 3: Optimized Model Training\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Split data\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_split.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val_split.shape[0]} samples\")\n",
    "\n",
    "# Add LightGBM for its speed and accuracy\n",
    "models = {\n",
    "    'LightGBM': lgb.LGBMRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        n_jobs=-1,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8\n",
    "    ),\n",
    "    'Random Forest (Fast)': RandomForestRegressor(\n",
    "        n_estimators=50, max_depth=15, min_samples_split=10,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'Linear Ridge': Ridge(alpha=0.1)\n",
    "}\n",
    "\n",
    "model_performance = {}\n",
    "scalers = {}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if 'Ridge' in name:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_split)\n",
    "        X_val_scaled = scaler.transform(X_val_split)\n",
    "        model.fit(X_train_scaled, y_train_split)\n",
    "        val_predictions = model.predict(X_val_scaled)\n",
    "        scalers[name] = scaler\n",
    "    else:\n",
    "        model.fit(X_train_split, y_train_split)\n",
    "        val_predictions = model.predict(X_val_split)\n",
    "    \n",
    "    print(f\"Training took: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    r2 = r2_score(y_val_split, val_predictions)\n",
    "    mae = mean_absolute_error(y_val_split, val_predictions)\n",
    "    errors = np.abs(val_predictions - y_val_split)\n",
    "    within_1pct = np.mean(errors < 0.01) * 100\n",
    "    within_5pct = np.mean(errors < 0.05) * 100\n",
    "    \n",
    "    model_performance[name] = {\n",
    "        'model': model, 'r2': r2, 'mae': mae,\n",
    "        'within_1pct': within_1pct, 'within_5pct': within_5pct\n",
    "    }\n",
    "    \n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    print(f\"  MAE: {mae:.6f}\")\n",
    "    print(f\"  Within 1%: {within_1pct:.1f}%\")\n",
    "    print(f\"  Within 5%: {within_5pct:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 4: EFFICIENT SEQUENCE PREDICTION (CORRECTED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 4: Fast Sequence Prediction\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Choose best model based on R²\n",
    "best_model_name = max(model_performance, key=lambda x: model_performance[x]['r2'])\n",
    "best_model_info = model_performance[best_model_name]\n",
    "best_model = best_model_info['model']\n",
    "best_scaler = scalers.get(best_model_name)\n",
    "\n",
    "print(f\"Best model: {best_model_name} (R²: {best_model_info['r2']:.4f})\")\n",
    "\n",
    "def predict_sequence(model, seed_data, scaler, num_predictions, window_size):\n",
    "    \"\"\"\n",
    "    CORRECTED VERSION: Generates a sequence of predictions, ensuring that the\n",
    "    data is converted to integers before creating bit-level features,\n",
    "    exactly matching the training process.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    current_data_floats = list(seed_data) # This history contains floats\n",
    "    \n",
    "    print(f\"Generating {num_predictions} predictions...\")\n",
    "    \n",
    "    for i in range(num_predictions):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  Progress: {i}/{num_predictions}\")\n",
    "        \n",
    "        # 1. Get the most recent float data for the window\n",
    "        window_floats = np.array(current_data_floats[-window_size:])\n",
    "        \n",
    "        # 2. !! CRITICAL FIX !! Convert the float window to integers\n",
    "        # This is the step that was missing and caused the error.\n",
    "        window_ints = (window_floats * (2**32)).astype(np.uint32)\n",
    "        \n",
    "        # 3. Create features for this single integer window\n",
    "        # We add a dummy integer at the end so the stride trick produces exactly one row\n",
    "        features_2d, _ = create_bit_features(np.append(window_ints, 0), window_size)\n",
    "        \n",
    "        # 4. Scale if necessary\n",
    "        if scaler:\n",
    "            features_2d = scaler.transform(features_2d)\n",
    "        \n",
    "        # 5. Predict the next value\n",
    "        next_pred_float = model.predict(features_2d)[0]\n",
    "        next_pred_float = np.clip(next_pred_float, 0.0, 1.0)\n",
    "        \n",
    "        # 6. Add the predicted *float* to our history and results\n",
    "        predictions.append(next_pred_float)\n",
    "        current_data_floats.append(next_pred_float)\n",
    "        \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Use the last part of the training data as the seed for prediction\n",
    "seed_sequence_floats = training_data[-WINDOW_SIZE:]\n",
    "\n",
    "# Generate predictions with the corrected function\n",
    "final_predictions = predict_sequence(\n",
    "    best_model, seed_sequence_floats, best_scaler, TEST_SIZE, WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 5: FAST VALIDATION AND RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: Final Results\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calculate final accuracy metrics\n",
    "errors = np.abs(final_predictions - test_targets)\n",
    "mae_final = np.mean(errors)\n",
    "rmse_final = np.sqrt(np.mean(errors**2))\n",
    "\n",
    "within_0_1pct = np.mean(errors < 0.001) * 100\n",
    "within_1pct = np.mean(errors < 0.01) * 100\n",
    "within_5pct = np.mean(errors < 0.05) * 100\n",
    "within_10pct = np.mean(errors < 0.1) * 100\n",
    "\n",
    "print(\"FINAL PREDICTION RESULTS:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Mean Absolute Error: {mae_final:.6f}\")\n",
    "print(f\"Root Mean Square Error: {rmse_final:.6f}\")\n",
    "print(f\"Predictions within 0.1%: {within_0_1pct:.1f}%\")\n",
    "print(f\"Predictions within 1%:   {within_1pct:.1f}%\")\n",
    "print(f\"Predictions within 5%:   {within_5pct:.1f}%\")\n",
    "print(f\"Predictions within 10%:  {within_10pct:.1f}%\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample Predictions vs True Values:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Index':<5} | {'Predicted':<12} | {'True Value':<12} | {'Error':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(min(15, len(final_predictions))):\n",
    "    print(f\"{i+1:<5d} | {final_predictions[i]:<12.8f} | {test_targets[i]:<12.8f} | {errors[i]:<12.8f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Fast visualization\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.suptitle(\"Analysis of PRNG (Mersenne Twister) Predictability\", fontsize=16, weight='bold')\n",
    "\n",
    "# Plot 1: Prediction Errors\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(errors, 'r-', alpha=0.7)\n",
    "plt.title('Absolute Prediction Errors', weight='bold')\n",
    "plt.xlabel('Prediction Step')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 2: Predictions vs. True Values\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(test_targets, final_predictions, alpha=0.6, s=25, edgecolors='k', c='blue')\n",
    "plt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "plt.title('Predictions vs. True Values', weight='bold')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Error Distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(errors, bins=40, alpha=0.75, color='green')\n",
    "plt.title('Error Distribution', weight='bold')\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 4: Sequence Comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(test_targets[:50], 'b-', label='True Sequence', linewidth=2)\n",
    "plt.plot(final_predictions[:50], 'r--', label='Predicted Sequence', linewidth=2)\n",
    "plt.title('Sequence Comparison (First 50 Steps)', weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 5: Model R² Comparison\n",
    "plt.subplot(2, 3, 5)\n",
    "model_names = list(model_performance.keys())\n",
    "r2_scores = [model_performance[name]['r2'] for name in model_names]\n",
    "colors = ['#4CAF50', '#FFC107', '#2196F3']\n",
    "bars = plt.bar(model_names, r2_scores, color=colors)\n",
    "plt.bar_label(bars, fmt='%.4f')\n",
    "plt.ylim(0, max(r2_scores) * 1.1)\n",
    "plt.title('Model Comparison (R² Score)', weight='bold')\n",
    "plt.ylabel('R² (Higher is better)')\n",
    "\n",
    "# Plot 6: Accuracy Thresholds\n",
    "plt.subplot(2, 3, 6)\n",
    "thresholds = ['< 0.1%', '< 1%', '< 5%', '< 10%']\n",
    "accuracies = [within_0_1pct, within_1pct, within_5pct, within_10pct]\n",
    "colors = ['#F44336', '#FF9800', '#FFEB3B', '#8BC34A']\n",
    "bars = plt.bar(thresholds, accuracies, color=colors)\n",
    "plt.bar_label(bars, fmt='%.1f%%')\n",
    "plt.ylim(0, 105)\n",
    "plt.title('Prediction Accuracy at Thresholds', weight='bold')\n",
    "plt.ylabel('% of Predictions within Error Margin')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SECURITY ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CRYPTOGRAPHIC SECURITY ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"ATTACK RESULTS:\")\n",
    "print(\"=\"*30)\n",
    "print(f\"✓ Achieved {within_1pct:.1f}% predictions within 1% accuracy.\")\n",
    "print(f\"✓ Training data used: {TRAINING_SIZE:,} samples.\")\n",
    "print(f\"✓ Best model: {best_model_name}.\")\n",
    "print(f\"✓ With a small set of outputs, future outputs can be predicted with high confidence.\")\n",
    "\n",
    "print(\"\\nSECURITY IMPLICATIONS:\")\n",
    "print(\"=\"*30)\n",
    "print(\"• The Mersenne Twister algorithm (used by `random`) is a PRNG, not a CSPRNG.\")\n",
    "print(\"• It is designed for simulation and modeling, NOT for security.\")\n",
    "print(\"• As demonstrated, its state can be learned, making it **COMPLETELY PREDICTABLE**.\")\n",
    "print(\"• Any system using `random.random()` for passwords, session tokens, cryptographic keys, or nonces is **fundamentally broken and insecure**.\")\n",
    "\n",
    "print(\"\\nCORRECT ALTERNATIVES FOR SECURITY:\")\n",
    "print(\"=\"*30)\n",
    "print(\"✓ Use Python's `secrets` module for generating secure tokens, passwords, etc.\")\n",
    "print(\"  `import secrets` -> `secrets.token_hex(16)`\")\n",
    "print(\"✓ Use `os.urandom()` for cryptographically secure random bytes from the OS.\")\n",
    "print(\"  `import os` -> `os.urandom(32)`\")\n",
    "\n",
    "# Demonstrate the difference\n",
    "import secrets\n",
    "import os\n",
    "\n",
    "print(\"\\nDEMONSTRATION OF SECURE VS. INSECURE:\")\n",
    "print(\"=\"*40)\n",
    "print(\"INSECURE (Predictable):\")\n",
    "random.seed(12345)\n",
    "print(f\"  > {random.random():.8f}, {random.random():.8f}, {random.random():.8f}\")\n",
    "\n",
    "print(\"\\nSECURE (Cryptographically Strong):\")\n",
    "print(f\"  > (from secrets) {secrets.SystemRandom().random():.8f}\")\n",
    "print(f\"  > (from os.urandom) A secure 16-byte token: {os.urandom(16).hex()}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ASSIGNMENT COMPLETE!\")\n",
    "print(f\"Key Lesson: NEVER use a standard Pseudorandom Number Generator (PRNG) for cryptographic applications. Always use a Cryptographically Secure PRNG (CSPRNG).\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eef2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
